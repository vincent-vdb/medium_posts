{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.utils import make_grid, draw_bounding_boxes\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.io import read_image\n",
    "import tqdm\n",
    "#import wandb\n",
    "\n",
    "from utils import Detect, MultiBoxLoss, od_collate_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelParameters:\n",
    "    \"\"\"Class with all the model parameters\"\"\"\n",
    "    batch_size: int = 16\n",
    "    lr: float = 0.001\n",
    "    scheduler_type: str = 'ReduceLROnPlateau'\n",
    "    lr_scheduler_patience: int = 10\n",
    "    epochs: int = 100\n",
    "    classes: list = field(default_factory=lambda: ['face'])\n",
    "    image_size: int = 128\n",
    "    detection_threshold: float = 0.5\n",
    "    blazeface_channels: int = 32\n",
    "    focal_loss: bool = False\n",
    "    model_path: str = 'weights/blazeface128.pt'\n",
    "    #use_wandb: bool = False\n",
    "    augmentation: dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = ModelParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, labels_path, image_size: int, augment: A.Compose = None):\n",
    "        self.labels_path = labels_path\n",
    "        self.labels = list(sorted(glob(f'{labels_path}/*')))\n",
    "        self.labels = [x for x in self.labels if os.stat(x).st_size != 0]\n",
    "        self.augment = augment\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            transforms.Resize((image_size, image_size))\n",
    "        ])\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images and masks\n",
    "        img_path = self.labels[idx].replace('labels', 'images')[:-3] + 'jpg'\n",
    "        img = plt.imread(img_path)\n",
    "        if len(img.shape) == 2 or img.shape[2] == 1:\n",
    "            # Handle grayscale images\n",
    "            img = np.stack((img,)*3, axis=-1)\n",
    "        if img.shape[2] == 4:\n",
    "            img = img[:, :, :3]\n",
    "        rescale_output = self.resize_and_pad(img, self.image_size)\n",
    "        img = rescale_output['image']\n",
    "        annotations = pd.read_csv(self.labels[idx], header=None, sep=' ')\n",
    "        labels = annotations.values[:, 0]\n",
    "        yolo_bboxes = annotations.values[:, 1:]\n",
    "        cx = yolo_bboxes[:, 0]\n",
    "        cy = yolo_bboxes[:, 1]\n",
    "        w = yolo_bboxes[:, 2]\n",
    "        h = yolo_bboxes[:, 3]\n",
    "        x1 = (cx - w / 2) * rescale_output['x_ratio'] + rescale_output['x_offset']\n",
    "        x2 = (cx + w / 2) * rescale_output['x_ratio'] + rescale_output['x_offset']\n",
    "        y1 = (cy - h / 2) * rescale_output['y_ratio'] + rescale_output['y_offset']\n",
    "        y2 = (cy + h / 2) * rescale_output['y_ratio'] + rescale_output['y_offset']\n",
    "        x1 = np.expand_dims(x1, 1)\n",
    "        x2 = np.expand_dims(x2, 1)\n",
    "        y1 = np.expand_dims(y1, 1)\n",
    "        y2 = np.expand_dims(y2, 1)\n",
    "        target = np.concatenate([x1, y1, x2, y2, labels.reshape(-1, 1)], axis=1).clip(0., 1.)\n",
    "        if self.augment is not None:\n",
    "            augmented = self.augment(image=img, bboxes=target)\n",
    "            img = augmented['image']\n",
    "            target = np.array(augmented['bboxes'])\n",
    "\n",
    "        return self.transform(img.copy()), np.clip(target, 0, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    @staticmethod\n",
    "    def resize_and_pad(img, target_size=128):\n",
    "        if img.shape[0] > img.shape[1]:\n",
    "            new_y = target_size\n",
    "            new_x = int(target_size * img.shape[1] / img.shape[0])\n",
    "        else:\n",
    "            new_y = int(target_size * img.shape[0] / img.shape[1])\n",
    "            new_x = target_size\n",
    "        output_img = cv2.resize(img, (new_x, new_y))\n",
    "        top = max(0, new_x - new_y) // 2\n",
    "        bottom = target_size - new_y - top\n",
    "        left = max(0, new_y - new_x) // 2\n",
    "        right = target_size - new_x - left\n",
    "        output_img = cv2.copyMakeBorder(\n",
    "            output_img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128, 128, 128)\n",
    "        )\n",
    "        # Compute labels values updates\n",
    "        x_ratio = new_x / target_size\n",
    "        y_ratio = new_y / target_size\n",
    "        x_offset = left / target_size\n",
    "        y_offset = top / target_size\n",
    "\n",
    "        return {'image': output_img, 'x_ratio': x_ratio, 'x_offset': x_offset, 'y_ratio': y_ratio, 'y_offset': y_offset}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment = A.Compose(\n",
    "    [\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, always_apply=True),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomCropFromBorders(\n",
    "            crop_left=0.05,\n",
    "            crop_right=0.05,\n",
    "            crop_top=0.05,\n",
    "            crop_bottom=0.05,\n",
    "            p=0.9,\n",
    "        ),\n",
    "        A.Affine(\n",
    "            rotate=(-30, 30),\n",
    "            scale=(0.8, 1.1),\n",
    "            keep_ratio=True,\n",
    "            translate_percent=(-0.05, 0.05),\n",
    "            cval=(128, 128, 128),\n",
    "            p=0.9,\n",
    "        ),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format='albumentations')\n",
    ")\n",
    "model_params.augmentation = augment.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'dataset/' \n",
    "train_dataset = MyDataset(\n",
    "    dataset_path + 'labels/train', \n",
    "    image_size=model_params.image_size,\n",
    "    augment=augment,\n",
    ")\n",
    "valid_dataset = MyDataset(\n",
    "    dataset_path + 'labels/val', \n",
    "    image_size=model_params.image_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_tuto import utils\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=model_params.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=od_collate_fn#utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, targets = next(iter(train_dataloader))  # get first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import draw_bounding_boxes\n",
    "idx = 7\n",
    "image, target = images[idx], targets[idx]\n",
    "\n",
    "classes = ['face', 'hand']\n",
    "labels = [classes[int(label)] for label in target[:, -1]]\n",
    "img_with_boxes = draw_bounding_boxes(((image*0.5 + 0.5)*255).to(torch.uint8),\n",
    "                                     target[:, :-1] * model_params.image_size,\n",
    "                                     labels)\n",
    "plt.imshow(img_with_boxes.permute(1, 2, 0).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pytorch_tuto import utils\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=model_params.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=od_collate_fn#utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test with ssd model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from blazeface import BlazeFace\n",
    "if model_params.image_size == 256:\n",
    "    model = BlazeFace(back_model=True)\n",
    "else:\n",
    "    model = BlazeFace()\n",
    "model.load_anchors('anchors.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device, focal=model_params.focal_loss, dbox_list=model.anchors)\n",
    "optimizer = optim.Adam(model.parameters(), lr=model_params.lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=model_params.lr_scheduler_patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(torch.randn(1, 3, model_params.image_size, model_params.image_size))\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        net,\n",
    "        dataloaders_dict,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        model_params,\n",
    "        device,\n",
    "):\n",
    "    net = net.to(device)\n",
    "\n",
    "    for epoch in range(model_params.epochs):\n",
    "        curr_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "        # Train\n",
    "        running_loss = 0.\n",
    "        running_loc_loss = 0.\n",
    "        running_class_loss = 0.\n",
    "        for images, targets in tqdm.tqdm(dataloaders_dict['train']):\n",
    "            images = images.to(device)\n",
    "            targets = [ann.to(device) for ann in targets]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss_l, loss_c = criterion(outputs, targets)\n",
    "            loss = loss_l + loss_c\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            running_loc_loss += loss_l.item()\n",
    "            running_class_loss += loss_c.item()\n",
    "\n",
    "        # Eval\n",
    "        net.eval()\n",
    "        val_loss = 0.\n",
    "        val_loc_loss = 0.\n",
    "        val_class_loss = 0.\n",
    "        with torch.no_grad():\n",
    "            for images, targets in dataloaders_dict['val']:\n",
    "                images = images.to(device)\n",
    "                targets = [ann.to(device) for ann in targets]\n",
    "                outputs = net(images)\n",
    "                loss_l, loss_c = criterion(outputs, targets)\n",
    "                loss = loss_l + loss_c\n",
    "                val_loss += loss.item()\n",
    "                val_loc_loss += loss_l.item()\n",
    "                val_class_loss += loss_c.item()\n",
    "\n",
    "        train_loss = running_loss / len(dataloaders_dict['train'])\n",
    "        train_loc_loss = running_loc_loss / len(dataloaders_dict['train'])\n",
    "        train_class_loss = running_class_loss / len(dataloaders_dict['train'])\n",
    "        val_loss = val_loss / len(dataloaders_dict['val'])\n",
    "        val_loc_loss = val_loc_loss / len(dataloaders_dict['val'])\n",
    "        val_class_loss = val_class_loss / len(dataloaders_dict['val'])\n",
    "        print(f'[{epoch + 1}] train loss: {train_loss:.3f} | val loss: {val_loss:.3f}')\n",
    "        print(f'train loc loss: {train_loc_loss:.3f} | train class loss: {train_class_loss:.3f}')\n",
    "        scheduler.step(val_loss)\n",
    "        # Save model\n",
    "        torch.save(net.state_dict(), model_params.model_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.anchors.min(axis=0), model.anchors.max(axis=0), model.anchors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(\n",
    "    model,\n",
    "    dataloaders_dict,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    model_params,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import onnx\n",
    "import onnx_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('original_blazeface_128.pt'))\n",
    "model.eval()\n",
    "model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_shape = (1, 3, model_params.image_size, model_params.image_size)\n",
    "output = model(torch.randn(input_shape))\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'original_blazeface_128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(model, torch.randn(input_shape), f'{model_name}.onnx', opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = onnx.load( f'{model_name}.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = onnx_tf.backend.prepare(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix for error from https://stackoverflow.com/questions/76839366/tf-rep-export-graphtf-model-path-keyerror-input-1\n",
    "from onnx import helper\n",
    "# Define a mapping from old names to new names\n",
    "name_map = {\"x.1\": \"x_1\"}\n",
    "\n",
    "# Initialize a list to hold the new inputs\n",
    "new_inputs = []\n",
    "\n",
    "# Iterate over the inputs and change their names if needed\n",
    "for inp in onnx_model.graph.input:\n",
    "    if inp.name in name_map:\n",
    "        # Create a new ValueInfoProto with the new name\n",
    "        new_inp = helper.make_tensor_value_info(name_map[inp.name],\n",
    "                                                inp.type.tensor_type.elem_type,\n",
    "                                                [dim.dim_value for dim in inp.type.tensor_type.shape.dim])\n",
    "        new_inputs.append(new_inp)\n",
    "    else:\n",
    "        new_inputs.append(inp)\n",
    "\n",
    "# Clear the old inputs and add the new ones\n",
    "onnx_model.graph.ClearField(\"input\")\n",
    "onnx_model.graph.input.extend(new_inputs)\n",
    "\n",
    "# Go through all nodes in the model and replace the old input name with the new one\n",
    "for node in onnx_model.graph.node:\n",
    "    for i, input_name in enumerate(node.input):\n",
    "        if input_name in name_map:\n",
    "            node.input[i] = name_map[input_name]\n",
    "onnx.save(onnx_model, f'{model_name}.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.export_graph(f'{model_name}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3/ TF to TFLite\n",
    "model_converter = tf.lite.TFLiteConverter.from_saved_model(f'{model_name}.tf')\n",
    "model_converter.target_spec.supported_ops = [\n",
    "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
    "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
    "]\n",
    "#model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = model_converter.convert()\n",
    "open(f'{model_name}.tflite', 'wb').write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
